# Story 1.26: AI Document Processing Optimization

## Status
Draft

## Story

**As a** user executing compliance assessments,
**I want** document processing to complete in seconds rather than minutes,
**so that** I can efficiently analyze multiple documents without long wait times.

## Acceptance Criteria

1. **Document Preprocessing Pipeline:**
   - All documents preprocessed ONCE per assessment execution (not per question)
   - Preprocessing extracts comprehensive summaries and embeddings
   - Preprocessed data cached in memory for assessment session duration
   - Cache key pattern: `assessment_docs:<assessment_id>`
   - Preprocessing runs in parallel across all documents

2. **Optimized Question Analysis:**
   - Each question analyzed using preprocessed document data
   - No redundant OpenAI API calls for same document content
   - Document relevance ranking using embeddings (cosine similarity)
   - Only top 3 most relevant documents sent to OpenAI per question
   - Reduced API calls: From 350 (7×50) to 57 (7 preprocessing + 50 analysis)

3. **Parallel Document Processing:**
   - All documents in preprocessing phase processed in parallel
   - Use Promise.all() for concurrent API calls (respecting rate limits)
   - Implement rate limit handling with exponential backoff
   - Maximum concurrent calls: 10 (configurable via env var)

4. **Smart Document Relevance Filtering:**
   - Calculate relevance score between question and each preprocessed document
   - Relevance algorithm: keyword matching + embedding similarity
   - Return top K documents (K=3 by default, configurable)
   - Include relevance score in evidence metadata

5. **Performance Metrics:**
   - Assessment execution time: <30 seconds (for 7 docs × 50 questions)
   - Document preprocessing time: <10 seconds for 7 documents
   - Question analysis time: <0.4 seconds average per question
   - API call reduction: 84% (350 → 57 calls)
   - Cost reduction: 84% on OpenAI API costs

6. **Backward Compatibility:**
   - Existing assessments continue to work without changes
   - Preprocessing is opt-in via assessment.service.ts method parameter
   - Fallback to old implementation if preprocessing fails
   - No breaking changes to public API

7. **Error Handling:**
   - Individual document preprocessing failure doesn't block entire assessment
   - Failed documents excluded from analysis with warning logged
   - Retry logic for transient OpenAI API failures (3 retries, exponential backoff)
   - Graceful degradation: use unparsed document text if preprocessing fails

8. **Monitoring & Observability:**
   - Log preprocessing time per document
   - Log total API calls per assessment
   - Log relevance scores for debugging
   - Emit metrics for Prometheus:
     - `assessment_preprocessing_duration_seconds` (histogram)
     - `assessment_api_calls_total` (counter with labels: type=preprocessing|analysis)
     - `document_relevance_score` (histogram)

9. **Configuration:**
   - Environment variables:
     - `AI_MAX_CONCURRENT_CALLS` (default: 10)
     - `AI_TOP_K_DOCUMENTS` (default: 3)
     - `AI_ENABLE_PREPROCESSING` (default: true)
     - `AI_PREPROCESSING_MODEL` (default: gpt-4)
   - Configurable via service constructor options

10. **Testing:**
    - Unit tests for relevance ranking algorithm
    - Unit tests for preprocessing pipeline
    - Integration test: 7 docs × 50 questions completes <30s
    - Integration test: API call count reduced by 84%
    - Integration test: preprocessing failure gracefully handled
    - Performance benchmark comparison (before/after)

## Tasks / Subtasks

- [ ] Create document preprocessing service (AC: 1)
  - [ ] Create `backend/src/services/document-preprocessing.service.ts`
  - [ ] Implement `preprocessDocumentsForAssessment()` method
  - [ ] Extract comprehensive summaries using OpenAI
  - [ ] Generate embeddings using OpenAI embeddings API
  - [ ] Store results in in-memory cache (Map<documentId, PreprocessingResult>)
  - [ ] Implement parallel processing with Promise.all()
  - [ ] Add error handling for individual document failures

- [ ] Implement document relevance ranking (AC: 4)
  - [ ] Create `backend/src/lib/document-relevance.ts`
  - [ ] Implement keyword extraction from question text
  - [ ] Implement cosine similarity for embedding comparison
  - [ ] Create hybrid scoring: (keyword_score × 0.3) + (embedding_score × 0.7)
  - [ ] Implement `rankDocumentsByRelevance()` function
  - [ ] Return top K documents with relevance scores

- [ ] Refactor AI analysis service for preprocessing (AC: 2)
  - [ ] Add `analyzeQuestionWithPreprocessedDocs()` method to ai-analysis.service.ts
  - [ ] Integrate document relevance ranking
  - [ ] Pass only top 3 relevant docs to OpenAI
  - [ ] Combine preprocessed summaries into single context
  - [ ] Update AnalysisResult to include relevance metadata
  - [ ] Keep existing `analyzeQuestion()` as fallback

- [ ] Update assessment execution flow (AC: 2, 6)
  - [ ] Modify `executeAssessment()` in assessment.service.ts
  - [ ] Add preprocessing phase before question loop
  - [ ] Pass preprocessed data to question analysis
  - [ ] Add fallback to old implementation if preprocessing fails
  - [ ] Maintain backward compatibility

- [ ] Implement rate limiting for parallel calls (AC: 3)
  - [ ] Create `backend/src/lib/rate-limiter.ts`
  - [ ] Implement semaphore pattern for max concurrent calls
  - [ ] Add exponential backoff for rate limit errors
  - [ ] Configure max concurrent calls via env var
  - [ ] Add retry logic (3 retries max)

- [ ] Add configuration management (AC: 9)
  - [ ] Add environment variables to .env.example
  - [ ] Update config/index.ts with new AI settings
  - [ ] Add validation for configuration values
  - [ ] Document configuration options in README

- [ ] Add monitoring and metrics (AC: 8)
  - [ ] Add preprocessing duration metric
  - [ ] Add API call counter with type labels
  - [ ] Add relevance score histogram
  - [ ] Log preprocessing time per document
  - [ ] Log total API calls per assessment

- [ ] Implement error handling (AC: 7)
  - [ ] Add try-catch around individual document preprocessing
  - [ ] Log warnings for failed documents
  - [ ] Continue assessment with successfully preprocessed docs
  - [ ] Add retry logic for OpenAI API failures
  - [ ] Implement graceful fallback to old implementation

- [ ] Write unit tests (AC: 10)
  - [ ] Test relevance ranking algorithm
  - [ ] Test keyword extraction
  - [ ] Test cosine similarity calculation
  - [ ] Test hybrid scoring formula
  - [ ] Test top K document selection
  - [ ] Test preprocessing pipeline
  - [ ] Test error handling for individual doc failures

- [ ] Write integration tests (AC: 10)
  - [ ] Test end-to-end assessment execution with preprocessing
  - [ ] Test 7 docs × 50 questions completes <30s
  - [ ] Test API call count reduction (350 → 57)
  - [ ] Test preprocessing failure graceful handling
  - [ ] Test fallback to old implementation
  - [ ] Test parallel processing with rate limiting

- [ ] Run performance benchmarks (AC: 5, 10)
  - [ ] Run existing benchmark script (before optimization)
  - [ ] Implement optimization
  - [ ] Run benchmark script again (after optimization)
  - [ ] Compare results: time, API calls, throughput
  - [ ] Document performance improvements
  - [ ] Verify 84% API call reduction
  - [ ] Verify 7x speedup achieved

- [ ] Update documentation (AC: 9)
  - [ ] Document preprocessing architecture in CLAUDE.md
  - [ ] Add configuration guide to README
  - [ ] Document relevance ranking algorithm
  - [ ] Add performance optimization section
  - [ ] Document migration path from old implementation

## Dev Notes

### Problem Statement

**Current Bottleneck:**
Located in `backend/src/services/ai-analysis.service.ts:409-441` and `backend/src/services/assessment.service.ts:1090-1159`

```
Current Flow (SLOW):
For each question (50 questions):
  For each document (7 documents):
    OpenAI API call to analyze document against question
    Total: 350 sequential API calls
    Time: ~175 seconds (@ 500ms per call)
    Cost: $3.50 per assessment
```

**Root Cause:**
- Same document analyzed repeatedly for every question (wasteful)
- Documents processed sequentially instead of parallel
- No caching or reuse of document analysis

### Solution Architecture

**New Flow (OPTIMIZED):**
```
Preprocessing Phase (ONCE per assessment):
  For each document (7 documents):
    OpenAI API call to extract comprehensive summary & embedding
    Process in parallel (7 concurrent calls)
    Total: 7 API calls
    Time: ~4 seconds (parallel execution)

Analysis Phase (per question):
  For each question (50 questions):
    1. Rank all documents by relevance to question (fast, in-memory)
    2. Select top 3 most relevant documents
    3. Single OpenAI API call with combined context from top 3 docs
    Total: 50 API calls
    Time: ~25 seconds (@ 500ms per call)

TOTAL: 57 API calls, ~29 seconds, $0.57 cost
IMPROVEMENT: 84% fewer calls, 7x faster, 84% cheaper
```

### Technical Implementation

**1. Document Preprocessing Service**

```typescript
// backend/src/services/document-preprocessing.service.ts

export interface PreprocessingResult {
  documentId: string;
  summary: string;
  embedding: number[]; // 1536 dimensions for text-embedding-ada-002
  keyTopics: string[];
  confidence: number;
  processingTimeMs: number;
}

export class DocumentPreprocessingService extends BaseService {
  private openai: OpenAI;

  async preprocessDocumentsForAssessment(
    documents: DatabaseDocument[],
    options: { maxConcurrent?: number } = {}
  ): Promise<Map<string, PreprocessingResult>> {
    const maxConcurrent = options.maxConcurrent || 10;
    const results = new Map<string, PreprocessingResult>();

    // Process documents in parallel with concurrency limit
    const chunks = this.chunkArray(documents, maxConcurrent);

    for (const chunk of chunks) {
      const chunkPromises = chunk.map(doc =>
        this.preprocessSingleDocument(doc).catch(error => {
          this.logger.warn('Document preprocessing failed', {
            documentId: doc.id,
            error: error.message
          });
          return null; // Don't fail entire batch
        })
      );

      const chunkResults = await Promise.all(chunkPromises);
      chunkResults.forEach(result => {
        if (result) results.set(result.documentId, result);
      });
    }

    return results;
  }

  private async preprocessSingleDocument(
    doc: DatabaseDocument
  ): Promise<PreprocessingResult> {
    const startTime = performance.now();

    // Extract comprehensive summary
    const summaryResponse = await this.openai.chat.completions.create({
      model: 'gpt-4',
      messages: [{
        role: 'system',
        content: 'Extract key compliance information, policies, procedures, controls, and evidence from this document. Provide a comprehensive summary.'
      }, {
        role: 'user',
        content: this.truncateContent(doc.parsedContent, 8000)
      }],
      temperature: 0.3,
      max_tokens: 1000
    });

    const summary = summaryResponse.choices[0].message.content;

    // Generate embedding for semantic search
    const embeddingResponse = await this.openai.embeddings.create({
      model: 'text-embedding-ada-002',
      input: summary
    });

    const embedding = embeddingResponse.data[0].embedding;

    const endTime = performance.now();

    return {
      documentId: doc.id,
      summary,
      embedding,
      keyTopics: this.extractKeyTopics(summary),
      confidence: 0.9,
      processingTimeMs: endTime - startTime
    };
  }
}
```

**2. Document Relevance Ranking**

```typescript
// backend/src/lib/document-relevance.ts

export interface RelevanceScore {
  documentId: string;
  score: number; // 0-1
  keywordScore: number;
  embeddingScore: number;
}

export class DocumentRelevanceRanker {
  /**
   * Rank documents by relevance to question
   */
  rankDocuments(
    question: DatabaseQuestion,
    preprocessedDocs: Map<string, PreprocessingResult>,
    topK: number = 3
  ): RelevanceScore[] {
    const questionEmbedding = this.generateQuestionEmbedding(question);
    const keywords = this.extractKeywords(question.text);

    const scores: RelevanceScore[] = [];

    for (const [docId, docData] of preprocessedDocs.entries()) {
      const keywordScore = this.calculateKeywordRelevance(
        keywords,
        docData.summary
      );

      const embeddingScore = this.cosineSimilarity(
        questionEmbedding,
        docData.embedding
      );

      // Hybrid scoring: 30% keywords, 70% semantic
      const score = (keywordScore * 0.3) + (embeddingScore * 0.7);

      scores.push({ documentId: docId, score, keywordScore, embeddingScore });
    }

    // Sort by score descending and return top K
    return scores.sort((a, b) => b.score - a.score).slice(0, topK);
  }

  private cosineSimilarity(vecA: number[], vecB: number[]): number {
    const dotProduct = vecA.reduce((sum, a, i) => sum + a * vecB[i], 0);
    const magnitudeA = Math.sqrt(vecA.reduce((sum, a) => sum + a * a, 0));
    const magnitudeB = Math.sqrt(vecB.reduce((sum, b) => sum + b * b, 0));
    return dotProduct / (magnitudeA * magnitudeB);
  }

  private calculateKeywordRelevance(
    keywords: string[],
    text: string
  ): number {
    const lowerText = text.toLowerCase();
    const matchCount = keywords.filter(kw =>
      lowerText.includes(kw.toLowerCase())
    ).length;
    return keywords.length > 0 ? matchCount / keywords.length : 0;
  }
}
```

**3. Updated AI Analysis Service**

```typescript
// backend/src/services/ai-analysis.service.ts

export class AIAnalysisService extends BaseService {
  /**
   * NEW: Analyze question with preprocessed documents
   */
  async analyzeQuestionWithPreprocessedDocs(
    question: DatabaseQuestion,
    preprocessedDocs: Map<string, PreprocessingResult>,
    websiteContent?: string,
    organizationData?: any,
    context?: ServiceContext
  ): Promise<ApiResponse<AnalysisResult>> {
    try {
      this.ensureOpenAIInitialized();

      // 1. Rank documents by relevance
      const ranker = new DocumentRelevanceRanker();
      const topDocs = ranker.rankDocuments(question, preprocessedDocs, 3);

      this.logger.info('Ranked documents by relevance', {
        questionId: question.id,
        topDocuments: topDocs.map(d => ({
          id: d.documentId,
          score: d.score
        }))
      });

      // 2. Combine top documents into single context
      const combinedContext = topDocs
        .map((doc, idx) => {
          const docData = preprocessedDocs.get(doc.documentId);
          return `[Document ${idx + 1} - Relevance: ${Math.round(doc.score * 100)}%]\n${docData.summary}`;
        })
        .join('\n\n---\n\n');

      // 3. Single API call with combined context
      const prompt = this.generatePrompt(question, organizationData);

      const response = await this.openai.chat.completions.create({
        model: 'gpt-4',
        messages: [{
          role: 'system',
          content: 'You are analyzing compliance questions against provided evidence.'
        }, {
          role: 'user',
          content: `${prompt}\n\nEvidence:\n${combinedContext}`
        }],
        temperature: 0.3,
        max_tokens: 800
      });

      const analysis = response.choices[0].message.content;

      // 4. Parse and score
      const scoring = await this.scoreFindings(question, topDocs);
      const explanation = await this.generateExplanation(question, topDocs, scoring.score);

      const result: AnalysisResult = {
        questionId: question.id,
        score: scoring.score,
        explanation,
        sourceReference: topDocs[0]?.documentId || null,
        confidence: scoring.confidence,
        evidence: topDocs.map(doc => ({
          source: preprocessedDocs.get(doc.documentId).filename,
          content: preprocessedDocs.get(doc.documentId).summary.substring(0, 200),
          relevance: doc.score
        })),
        status: this.determineAnswerStatus(scoring.score, topDocs)
      };

      return this.createResponse(true, result, 'Question analyzed successfully');

    } catch (error) {
      this.logger.error('Failed to analyze question with preprocessed docs', {
        questionId: question.id,
        error: error.message
      });
      throw this.createError('Failed to analyze question', 500, 'ANALYSIS_ERROR');
    }
  }

  // Keep existing analyzeQuestion() for backward compatibility
  async analyzeQuestion(...args): Promise<ApiResponse<AnalysisResult>> {
    // Existing implementation unchanged
  }
}
```

**4. Updated Assessment Execution Flow**

```typescript
// backend/src/services/assessment.service.ts

async executeAssessment(
  assessmentId: string,
  documentIds: string[],
  context?: ServiceContext
): Promise<ApiResponse<ExecutionResult>> {
  try {
    const preprocessingService = new DocumentPreprocessingService();
    const aiAnalysis = new AIAnalysisService();

    // Load assessment data
    const assessment = await this.loadAssessmentWithDocuments(assessmentId);
    const selectedDocuments = this.filterSelectedDocuments(assessment, documentIds);

    // NEW: Preprocessing phase (parallel, once per assessment)
    this.logger.info('Starting document preprocessing', {
      assessmentId,
      documentCount: selectedDocuments.length
    });

    const preprocessingStart = performance.now();
    const preprocessedDocs = await preprocessingService.preprocessDocumentsForAssessment(
      selectedDocuments,
      { maxConcurrent: parseInt(process.env.AI_MAX_CONCURRENT_CALLS || '10') }
    );
    const preprocessingEnd = performance.now();

    this.logger.info('Document preprocessing completed', {
      assessmentId,
      documentsProcessed: preprocessedDocs.size,
      timeMs: Math.round(preprocessingEnd - preprocessingStart)
    });

    // Collect all questions
    const allQuestions = this.collectAllQuestions(assessment.template);

    // Track progress
    let successCount = 0;
    let failureCount = 0;

    // NEW: Process questions using preprocessed data
    const batchSize = 5;
    for (let i = 0; i < allQuestions.length; i += batchSize) {
      const batch = allQuestions.slice(i, i + batchSize);

      const batchPromises = batch.map(async (question) => {
        try {
          // Use NEW method with preprocessed docs
          const analysisResult = await aiAnalysis.analyzeQuestionWithPreprocessedDocs(
            question,
            preprocessedDocs,
            websiteContent,
            assessment.organization,
            context
          );

          if (analysisResult.success) {
            await answerService.createAnswer(
              assessmentId,
              question.id,
              analysisResult.data.score,
              analysisResult.data.explanation,
              analysisResult.data.sourceReference,
              analysisResult.data.status,
              context
            );
            successCount++;
          } else {
            failureCount++;
          }
        } catch (error) {
          this.logger.error('Question analysis failed', {
            questionId: question.id,
            error: error.message
          });
          failureCount++;
        }
      });

      await Promise.all(batchPromises);

      // Log progress
      this.logger.info('Batch processing progress', {
        assessmentId,
        processed: i + batch.length,
        total: allQuestions.length
      });
    }

    // Continue with gap generation, risk analysis, etc...

  } catch (error) {
    // Fallback to old implementation if preprocessing fails
    this.logger.warn('Preprocessing failed, falling back to legacy implementation', {
      assessmentId,
      error: error.message
    });

    return this.executeAssessmentLegacy(assessmentId, documentIds, context);
  }
}
```

### Relevant Source Tree

**New Files:**
- `backend/src/services/document-preprocessing.service.ts` - Document preprocessing logic
- `backend/src/lib/document-relevance.ts` - Relevance ranking algorithm
- `backend/src/lib/rate-limiter.ts` - Rate limiting for parallel API calls
- `backend/scripts/assessment-performance-benchmark.ts` - Performance benchmarking (CREATED)

**Modified Files:**
- `backend/src/services/ai-analysis.service.ts` - Add preprocessed doc analysis method
- `backend/src/services/assessment.service.ts` - Update execution flow
- `backend/src/config/index.ts` - Add AI configuration
- `backend/.env.example` - Add new environment variables

### Performance Benchmarks

**Before Optimization:**
- 7 documents × 50 questions = 350 API calls
- Sequential processing: ~175 seconds
- Cost: $3.50 per assessment

**After Optimization:**
- 7 preprocessing + 50 analysis = 57 API calls
- Parallel + optimized: ~29 seconds
- Cost: $0.57 per assessment

**Improvements:**
- ⚡ **7x faster** (175s → 29s)
- 💰 **84% cost reduction** ($3.50 → $0.57)
- 📉 **84% fewer API calls** (350 → 57)

### Testing

**Location:**
- `backend/tests/integration/document-preprocessing.spec.ts` (new)
- `backend/tests/integration/assessment-execution-optimized.spec.ts` (new)
- `backend/tests/unit/document-relevance.spec.ts` (new)

**Test Coverage Target:** ≥85%

**Key Test Cases:**
1. Preprocessing completes successfully for 7 documents
2. Relevance ranking returns top 3 docs correctly
3. API call count reduced from 350 to 57
4. Assessment execution completes <30 seconds
5. Individual document preprocessing failure doesn't block assessment
6. Fallback to legacy implementation works
7. Parallel processing respects rate limits
8. Embeddings correctly calculate cosine similarity

### Integration Points

**Dependencies:**
- OpenAI API (gpt-4 for summaries, text-embedding-ada-002 for embeddings)
- Existing Redis cache (optional: cache preprocessing results)
- Existing Prisma database models (no changes needed)

**API Endpoints (No Changes):**
- Existing assessment execution endpoints work unchanged
- Optimization is transparent to API consumers

### Migration Path

**Phase 1: Add New Services (No Breaking Changes)**
- Implement preprocessing service
- Implement relevance ranking
- Add new analysis method
- Keep old implementation intact

**Phase 2: Gradual Rollout**
- Enable preprocessing via feature flag
- Monitor performance and errors
- Compare old vs new metrics

**Phase 3: Full Migration**
- Make preprocessing default
- Remove feature flag
- Deprecate old implementation (keep as fallback)

## Change Log

| Date       | Version | Description                          | Author        |
|------------|---------|--------------------------------------|---------------|
| 2025-10-13 | 1.0     | Initial story created from benchmark | James (Dev)   |

## Dev Agent Record

### Agent Model Used
Claude Sonnet 4.5 (claude-sonnet-4-5-20250929)

### Debug Log References
_To be populated by dev agent during implementation_

### Completion Notes

**Implementation Status: CORE COMPLETE** ✅

Performance baseline established and core optimization components implemented:

**Performance Baseline (Benchmark Results):**
- Current user scenario (7 docs × 50 questions): **46.1 seconds**, 350 API calls, $3.50 cost
- Expected with optimization: **~8 seconds**, 57 API calls, $0.57 cost
- **Projected improvement: 5.8x faster, 84% cost reduction**
- Baseline documented in `docs/performance-baseline.md`
- Raw benchmark data: `backend/benchmark-results.json`

**Core Components Implemented:**
1. ✅ Document preprocessing service with parallel processing
2. ✅ Document relevance ranking with hybrid scoring (keyword + embedding)
3. ✅ Updated AI analysis service with preprocessed document method
4. ✅ Environment configuration added to .env.example
5. ✅ Performance benchmark tool and baseline documentation

**Remaining Integration Work:**
- Wire up optimized flow in `assessment.service.ts` executeAssessment method
- Add feature flag for gradual rollout
- Write unit tests for relevance ranking algorithm
- Write integration tests for end-to-end optimization
- Add Prometheus metrics for monitoring

**Key Integration Points:**
- `assessment.service.ts:961-1326` - executeAssessment method needs preprocessing phase
- `ai-analysis.service.ts:190` - New analyzeQuestionWithPreprocessedDocs method ready
- Feature flag check: `process.env.AI_ENABLE_PREPROCESSING === 'true'`

### File List
- ✅ Created: `backend/scripts/assessment-performance-benchmark.ts` (320 lines)
- ✅ Created: `backend/src/services/document-preprocessing.service.ts` (381 lines)
- ✅ Created: `backend/src/lib/document-relevance.ts` (338 lines)
- ✅ Modified: `backend/src/services/ai-analysis.service.ts` (+190 lines - new method)
- ✅ Modified: `backend/src/services/assessment.service.ts` (added import)
- ✅ Created: `docs/performance-baseline.md` (189 lines)
- ✅ Modified: `backend/.env.example` (added AI optimization vars)
- 📝 Planned: `backend/src/lib/rate-limiter.ts` (optional - can use built-in Promise.all chunking)
- 📝 Planned: `backend/tests/unit/document-relevance.spec.ts`
- 📝 Planned: `backend/tests/integration/assessment-execution-optimized.spec.ts`

## QA Results

_To be populated by QA agent after implementation review_
